# vllm-skills ğŸš€

> Agent skills for vLLM â€” production-ready tools for LLM agents

## Why vllm-skills?

| Feature | Benefit |
|---------|---------|
| ğŸ”§ Pre-built skills | Deployment, coding, web, data processing |
| ğŸ³ Sandboxed execution | Safe code execution via Docker |
| âš¡ Optimized for vLLM | Template fixes + FSM warmup for guided decoding |
| ğŸ”Œ Easy integration | Mount alongside your vLLM server |
| ğŸ“– Recipe integration | Works with vllm-project/recipes |

## Quickstart

```bash
pip install vllm-skills
```

## Architecture

The vllm-skills framework provides a modular architecture for building and deploying agent skills:

- **core/base.py** - Abstract base class for skills with metadata and execution interface
- **core/registry.py** - Skill discovery and loading system
- **core/sandbox.py** - Safe execution environments (Docker/Local)
- **library/** - Collection of built-in skills
- **vllm_utils/** - vLLM-specific optimizations

## Available Skills

### ğŸš€ Deployment Assistant

Intelligent vLLM deployment helper that:
- Auto-detects hardware and environment (GPU, CPU, RAM, CUDA)
- Suggests optimal configuration based on your system
- Integrates with [vllm-project/recipes](https://github.com/vllm-project/recipes)
- Troubleshoots common issues (OOM, CUDA errors, kernel issues)
- Provides configuration presets (high throughput, low latency, memory constrained)

### Coming Soon

- **Coding Skills** - Code generation, refactoring, and analysis
- **Web Skills** - Web scraping, API interaction, and data extraction
- **Data Skills** - Data processing, transformation, and analysis

## Integration with vllm-project/recipes

This project works alongside [vllm-project/recipes](https://github.com/vllm-project/recipes) by:

- Mapping popular models (DeepSeek, Qwen, Llama, Mistral, etc.) to their deployment recipes
- Providing hardware compatibility matrices
- Auto-generating optimal launch commands based on your environment
- Troubleshooting using recipe-specific knowledge

## Usage Example

```python
from vllm_skills.client import SkillEnabledClient

# Initialize client with deployment skill
client = SkillEnabledClient(
    base_url="http://localhost:8000/v1",
    skills=["deployment"]
)

# Use the deployment assistant
response = client.chat_with_skills(
    messages=[
        {"role": "user", "content": "Help me deploy Llama 3.3 70B on my system"}
    ]
)
```

## Project Structure

```
vllm-skills/
â”œâ”€â”€ vllm_skills/          # Main package
â”‚   â”œâ”€â”€ core/             # Core framework
â”‚   â”œâ”€â”€ library/          # Skill library
â”‚   â”‚   â”œâ”€â”€ deployment/   # Deployment assistant
â”‚   â”‚   â”œâ”€â”€ coding/       # Coding skills
â”‚   â”‚   â”œâ”€â”€ web/          # Web skills
â”‚   â”‚   â””â”€â”€ data/         # Data skills
â”‚   â”œâ”€â”€ vllm_utils/       # vLLM optimizations
â”‚   â””â”€â”€ client.py         # Client wrapper
â”œâ”€â”€ examples/             # Usage examples
â””â”€â”€ tests/               # Test suite
```

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:

- Adding new skills
- Skill structure template
- Testing requirements
- PR process

## License

Apache 2.0 - see [LICENSE](LICENSE) for details.

## Resources

- [vLLM Documentation](https://docs.vllm.ai/)
- [vLLM Recipes](https://github.com/vllm-project/recipes)
- [GitHub Discussions](https://github.com/vllm-project/vllm-skills/discussions)
